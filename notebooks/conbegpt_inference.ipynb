{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ConBEGPT Inference\n",
    "In this notebook it is described how easily you could use our model to detoxificate text.\n",
    "\n",
    "Important note: The notebook use pseudo-absolute path and should be launched only once. So If you want to launch it second time, restart the kernel."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-11-03T02:50:35.215061Z",
     "end_time": "2023-11-03T02:50:35.215614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leon/Projects/Programming/Study/Python/ML_Inno/PMLDL/PML_ASS_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Upcast the path to the src folder\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def manual_seed(seed):\n",
    "    \"\"\"\n",
    "    Function to set the seed value for reproducibility\n",
    "    :param seed: seed value\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # PyTorch manual seed\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # NumPy manual seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Set the seed value\n",
    "seed = 42\n",
    "\n",
    "# Call the manual seeding function\n",
    "manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download model parameters\n",
    "Simply use the automatic function to collect, download and install all the files (weights, vocabularies, etc.) in correct path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 data is already downloaded.\n",
      "Conditional BERT vocabs are already downloaded.\n",
      "Estimator is already downloaded.\n",
      "Datasets are already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import src.models.downloader as Downoader\n",
    "\n",
    "# Download all the model parameters automatically\n",
    "Downoader.download_data(download_gpt2=True, download_conbert=True, download_datasets=True, download_estimator=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-03T02:50:35.215138Z",
     "end_time": "2023-11-03T02:50:35.293947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Conditional Bert model...\n",
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT vocabularies...\n",
      "Loading Estimator model...\n",
      "Loading estimator vocab...\n",
      "Loading estimator model...\n",
      "Loading GPT-2 model...\n"
     ]
    }
   ],
   "source": [
    "from models.conbegpt import Conbergpt\n",
    "\n",
    "# Get the device\n",
    "device = 'cpu'\n",
    "\n",
    "# Paths to load the components of ConBEGPT model\n",
    "conbert_dir = 'models/Conbert'\n",
    "gpt2_dir = 'models/Gpt2'\n",
    "estimator_dir = 'models/Estimator'\n",
    "\n",
    "model = Conbergpt(device, conbert_dir, estimator_dir, gpt2_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-03T02:50:35.295473Z",
     "end_time": "2023-11-03T02:50:48.310528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "' Screw '"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.detoxicate(\"Fuck\", max_len=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-03T02:51:02.710443Z",
     "end_time": "2023-11-03T02:51:04.612918Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
